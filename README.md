# setting up your environment and repository with uv

> If you were starting from scratch, it would be ideal to start by creating a new project (the title of the project will be the title of the root directory) using: 
```
uv init
```
> you may notice a .venv folder - this is a virtual environment that isolates dependencies from your system, enabling version control and reproducibility across environments.

in our case since you have cloned my repository, you do not have to run uv init. Run: 
```
uv venv
```
this creates a .venv folder for you and next you have to activate this virtual environment. Run:
```
source .venv/bin/activate
```
to deactivate your virtual environment simply type `deactivate`

# downloading dependencies

Next install all the dependencies found in the uv.lock file provided by running:
```
uv sync
```

# structuring your repository for data science experiments

A machine learning experiment usually has the following parts. Here's how this repository is organized:

```
dac_proj/
├── conf/                    # Configuration files
│   ├── process_data.yaml    # Data processing parameters
│   └── train_model.yaml     # Model training hyperparameters
├── data/                    # Data directory
│   ├── raw/                 # Original, immutable data
│   │   └── german_credit_combined.csv
│   └── processed/           # Cleaned, transformed data
│       ├── X_train_transformed.csv
│       ├── y_train_transformed.csv
│       ├── X_test_transformed.csv
│       └── y_test_transformed.csv
├── src/                     # Source code
│   ├── __init__.py          # Makes src a Python package
│   ├── datapipeline.py      # Data loading and preprocessing
│   ├── mlp_datapipeline.py  # MLP-specific data pipeline
│   ├── model.py             # Model definitions
│   └── mlp.py               # MLP model implementation
├── eda.ipynb                # Exploratory data analysis notebook
├── pyproject.toml           # Project metadata and dependencies
├── uv.lock                  # Locked dependency versions
└── README.md                # This file
```

## key components

### conf/
Store configuration files (YAML, JSON) that define parameters for your experiments. This separates configuration from code, making it easier to run experiments with different settings.

### data/
- **raw/**: Original data files. Never modify these directly.
- **processed/**: Transformed data ready for modeling. Generated by your data pipeline scripts.

### src/
Python modules containing your core logic:
- Data pipelines for loading and transforming data
- Model definitions and training logic
- Utility functions

### notebooks
Jupyter notebooks (like `eda.ipynb`) are great for exploration and visualization. Keep production code in `src/`.

## running experiments

1. Process your data:
```bash
python src/datapipeline.py
```

2. Train your model:
```bash
python src/model.py
```

# questions and faqs