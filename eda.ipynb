{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56659742",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9fc0cf",
   "metadata": {},
   "source": [
    "As students, we often do not consider Exploratory Data Analysis (EDA) in depth, as our datasets are typically well-curated and features are mostly independent. However, in real-world scenarios, EDA can account for up to 80% of a data scientistâ€™s workload, and for data analysts, it is often their primary focus. A structured approach to EDA can be outlined as follows:\n",
    "\n",
    "- Univariate analysis and outlier detection\n",
    "- Multivariate analysis, starting with correlation analysis among numerical features and extending to cross-tabulation of numeric and categorical variables\n",
    "\n",
    "It is important to formulate hypotheses about your data and validate them, either through statistical techniques or other appropriate methods. \n",
    "\n",
    "For example, a hypothesis could be: \"The average resale price of 5-room flats is significantly higher than that of 3-room flats.\" \n",
    "\n",
    "To validate this, you could use statistical tests like the independent t-test or ANOVA to compare the means between flat types.\n",
    "\n",
    "Other appropriate methods for hypothesis validation include data visualization (e.g., box plots, scatter plots to observe distributions and trends), machine learning models to assess feature importance, or domain-driven checks such as consulting policy changes and timelines that could explain variations in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ebffb",
   "metadata": {},
   "source": [
    "### on domain knowledge:\n",
    "Domain knowledge provides critical insights into the data that may not be apparent through data analysis techniques alone. For example, policy changes affecting public housing eligibility or pricing can explain shifts or patterns in resale flat prices that are not readily observable from the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.mlp_datapipeline import DataPipeline\n",
    "from src.mlp import MLPTwoLayers as MLP\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "statlog_german_credit_data = fetch_ucirepo(id=144) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = statlog_german_credit_data.data.features \n",
    "y = statlog_german_credit_data.data.targets \n",
    "\n",
    "\n",
    "\n",
    "with open(\"statlog_german_credit_metadata.txt\", \"w\") as meta_file:\n",
    "    for key, value in statlog_german_credit_data.metadata.items():\n",
    "        meta_file.write(f\"{key}:\\n\")\n",
    "        if key == 'additional_info' and isinstance(value, dict):\n",
    "            # Pretty print the additional_info dictionary with proper formatting\n",
    "            for info_key, info_value in value.items():\n",
    "                meta_file.write(f\"  {info_key}: \")\n",
    "                if isinstance(info_value, str):\n",
    "                    # Replace escape sequences with actual newlines and tabs for readability\n",
    "                    formatted_value = info_value.replace('\\\\r\\\\n', '\\n').replace('\\\\t', '    ')\n",
    "                    meta_file.write(f\"\\n{formatted_value}\\n\")\n",
    "                else:\n",
    "                    meta_file.write(f\"{info_value}\\n\")\n",
    "        else:\n",
    "            meta_file.write(f\"{value}\")\n",
    "        meta_file.write(\"\\n\\n\")\n",
    "print(\"Metadata saved to statlog_german_credit_metadata.txt\")\n",
    "  \n",
    "# variable information \n",
    "print(statlog_german_credit_data.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6ab3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c376b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport \n",
    "# Combine features and target for profiling\n",
    "data_combined = X.copy()\n",
    "data_combined['class'] = y\n",
    "\n",
    "# Save combined dataset to data/raw\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "data_combined.to_csv(\"data/raw/german_credit_combined.csv\", index=False)\n",
    "\n",
    "# Generate profile report\n",
    "profile = ProfileReport(data_combined, title=\"German Credit Data Profiling Report\", explorative=True)\n",
    "profile.to_file(\"german_credit_profile_report.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3200752",
   "metadata": {},
   "source": [
    "### Do your train-test split before transforming your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ca413",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,      \n",
    "    random_state=42,    \n",
    "    stratify=y  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c342380",
   "metadata": {},
   "source": [
    "### Instantiate your pipeline and transform your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d7e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DataPipeline()\n",
    "X_train_transformed, y_train_transformed = pipeline.fit_transform(X_train, y_train)\n",
    "X_test_transformed, y_test_transformed = pipeline.transform(X_test, y_test)\n",
    "\n",
    "# Save transformed data to data/processed\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "pd.DataFrame(X_train_transformed).to_csv(\"data/processed/X_train_transformed.csv\", index=False)\n",
    "pd.DataFrame(y_train_transformed).to_csv(\"data/processed/y_train_transformed.csv\", index=False)\n",
    "pd.DataFrame(X_test_transformed).to_csv(\"data/processed/X_test_transformed.csv\", index=False)\n",
    "pd.DataFrame(y_test_transformed).to_csv(\"data/processed/y_test_transformed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428bee8e",
   "metadata": {},
   "source": [
    "### Instantiate your model class with the appropriate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e32687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_size=X_train_transformed.shape[1], hidden_size=64, output_size=2, learning_rate=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95994953",
   "metadata": {},
   "source": [
    "### Utilise the `Forward` method to take the set of features and return the probabilities of each class for that input datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba94ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.forward(X_train_transformed[[0]])\n",
    "print(f\"Predictions on 1st sample: {preds}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba92a9",
   "metadata": {},
   "source": [
    "### Utilise the `loss` method, which takes in the predicted probability and actual label and returns the loss for that input datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = model.loss(preds, y_train_transformed[[0]])\n",
    "print(f\"Train loss on 1st sample: {train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2890c959",
   "metadata": {},
   "source": [
    "### Utilise the `backward` method to update weights and biases using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the value of the loss function before backpropagation\n",
    "# Our optimiser is stochastic gradient descent (SGD) since the model is being updated after\n",
    "# each individual sample (batch_size=1). The script iterates through samples\n",
    "# one by one, computes the forward pass, calculates loss, and immediately\n",
    "# calls backward() to update weights for each sample before moving to the next.\n",
    "test_loss = 0\n",
    "for i in range(X_test_transformed.shape[0]):\n",
    "    test_loss += model.loss(model.forward(X_test_transformed[[i]]), y_test_transformed[[i]])\n",
    "    model.backward(y_test_transformed[[i]])\n",
    "print(f\"Test loss: {test_loss / X_test_transformed.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271311e4",
   "metadata": {},
   "source": [
    "### Calculate test loss after training (post-backpropagation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa3e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final test loss\n",
    "test_loss = 0\n",
    "for i in range(X_test_transformed.shape[0]):\n",
    "    test_loss += model.loss(model.forward(X_test_transformed[[i]]), y_test_transformed[[i]])\n",
    "print(test_loss / X_test_transformed.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b9047",
   "metadata": {},
   "source": [
    "Now suppose we want to run 100 epochs of training. We can do this by iterating through the training data 100 times and updating the model's weights and biases after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df08861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        epoch_train_loss = 0\n",
    "        for i in range(X_train_transformed.shape[0]):\n",
    "            preds = model.forward(X_train_transformed[[i]])\n",
    "            epoch_train_loss += model.loss(preds, y_train_transformed[[i]])\n",
    "            model.backward(y_train_transformed[[i]])\n",
    "        train_losses.append(epoch_train_loss / X_train_transformed.shape[0])\n",
    "\n",
    "        # Test loss (no backward pass)\n",
    "        epoch_test_loss = 0\n",
    "        for i in range(X_test_transformed.shape[0]):\n",
    "            preds = model.forward(X_test_transformed[[i]])\n",
    "            epoch_test_loss += model.loss(preds, y_test_transformed[[i]])\n",
    "        test_losses.append(epoch_test_loss / X_test_transformed.shape[0])\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Train={train_losses[-1]:.4f}, Test={test_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773df252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting your loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_epochs + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, n_epochs + 1), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Training and Test Loss Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2aa91f",
   "metadata": {},
   "source": [
    "How do we interpret our Training and Test loss curves?\n",
    "\n",
    "* between epoch 0-100 both losses decrease together (model is learning generalizable patterns)\n",
    "* epoch 100-200 curves start to diverge (model begins to memorize training data)\n",
    "* epoch 200 and beyond train loss keeps decreasing but test loss plateaus (model is overfit)\n",
    "\n",
    "a practical takeaway would be to consider **early stopping** - saving the model weights when the test loss stops improving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415baad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "y_pred = []\n",
    "for i in range(X_test_transformed.shape[0]):\n",
    "    preds = model.forward(X_test_transformed[[i]])\n",
    "    y_pred.append(np.argmax(preds))\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "cm = confusion_matrix(y_test_transformed, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=['Good Credit', 'Bad Credit'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test_transformed)\n",
    "print(f\"Test Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea485ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
